{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAP9002/COMP34812-NLU-NLI/blob/main/NLU_Method_C/NLI_Transformer_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer-based NLI Solution: Demo Notebook\n",
        "\n",
        "**Instructions:**\n",
        "1. Install dependencies (first cell).\n",
        "2. Upload your `test.csv` file (premise, hypothesis).\n",
        "3. Run all cells top to bottom.\n",
        "4. Predictions will be saved in a csv file for submission."
      ],
      "metadata": {
        "id": "ZzcV5PjeP7T-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements Packages"
      ],
      "metadata": {
        "id": "z-L8M2HEQfZ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVu8t4c4P5f1"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy tensorflow transformers huggingface_hub --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, TFRobertaModel, TFRobertaForSequenceClassification\n",
        "from huggingface_hub import snapshot_download"
      ],
      "metadata": {
        "id": "oUGY4_sXQZGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "INPUT_FILE_PATH = \"./test.csv\"\n",
        "OUTPUT_CSV_FILE = \"predictions.csv\"\n",
        "\n",
        "MODEL_REPO = \"aap9002/NLI-Transformer-Ensemble-Model\"\n",
        "MODEL_FILE = \"ensamble_model_weights_and_arch.h5\""
      ],
      "metadata": {
        "id": "-bmW95XTQc2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Load Model"
      ],
      "metadata": {
        "id": "66Sl_RGNQUmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set HF_TOKEN in your enviroment\n",
        "\n",
        "snapshot_download(\n",
        "    repo_id=MODEL_REPO,\n",
        "    allow_patterns=f\"*{MODEL_FILE}\",\n",
        "    local_dir='./'\n",
        "    )"
      ],
      "metadata": {
        "id": "ZnQeVGpHQT8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_model = tf.keras.models.load_model(\n",
        "    MODEL_FILE,\n",
        "    custom_objects={\n",
        "        'TFRobertaForSequenceClassification': TFRobertaForSequenceClassification,\n",
        "        'TFRobertaModel': TFRobertaModel\n",
        "        }\n",
        ")"
      ],
      "metadata": {
        "id": "IvOiWUsBSIFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Test Data"
      ],
      "metadata": {
        "id": "29DnJww1RPvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = pd.read_csv(INPUT_FILE_PATH)\n",
        "test_set.head()"
      ],
      "metadata": {
        "id": "HAv-Y3IeQwkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_large_tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-large\")\n",
        "roberta_base_tokenizer = AutoTokenizer.from_pretrained( 'FacebookAI/roberta-base')"
      ],
      "metadata": {
        "id": "2xmxMEs2RSk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ensemble_dataset(df, nli_tokenizer = roberta_large_tokenizer, sim_tokenizer = roberta_base_tokenizer, batch_size=BATCH_SIZE):\n",
        "  premises = df['premise'].tolist()\n",
        "  hypotheses = df['hypothesis'].tolist()\n",
        "  labels = df['label'].tolist()\n",
        "\n",
        "  inputs_nli = roberta_large_tokenizer(\n",
        "      premises,\n",
        "      hypotheses,\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      return_tensors=\"tf\"\n",
        "  )\n",
        "\n",
        "  # Tokenize each set of sentences separately\n",
        "  inputs_a = sim_tokenizer(\n",
        "        premises,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors='np'\n",
        "  )\n",
        "\n",
        "  inputs_b = sim_tokenizer(\n",
        "        hypotheses,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors='np'\n",
        "  )\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\n",
        "            'input_ids_nli': inputs_nli['input_ids'],\n",
        "            'attention_mask_nli': inputs_nli['attention_mask'],\n",
        "            'input_ids_a': inputs_a['input_ids'],\n",
        "            'attention_mask_a': inputs_a['attention_mask'],\n",
        "            'input_ids_b': inputs_b['input_ids'],\n",
        "            'attention_mask_b': inputs_b['attention_mask']\n",
        "        },\n",
        "        tf.one_hot(labels, depth=2)  # For binary classification (adjust depth for multi-class)\n",
        "  ))\n",
        "\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "ZFmYizN1RhIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ensemble_dataset = create_ensemble_dataset(test_set)"
      ],
      "metadata": {
        "id": "1xpZsr6qRjX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run model predicitons"
      ],
      "metadata": {
        "id": "VISwdFHYR3Sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = ensemble_model.predict(test_ensemble_dataset)\n",
        "prediction_labels = predictions.argmax(axis=-1)"
      ],
      "metadata": {
        "id": "e5k-d2sdR25v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output Predictions"
      ],
      "metadata": {
        "id": "4RtSRpppSUE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['prediction']\n",
        "\n",
        "df = pd.DataFrame(prediction_labels, columns=columns)\n",
        "\n",
        "df.to_csv(OUTPUT_CSV_FILE, index=False)"
      ],
      "metadata": {
        "id": "MANECCEtSklU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}