{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnOgaPA41fkr"
      },
      "source": [
        "# COMP34812 Natural Language Understanding Courseworklow key lemming an stemming\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmSUwder1fkt"
      },
      "source": [
        "## Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FqR9evYQ1fku",
        "outputId": "f35fd907-8027-4e57-c0c3-42bca35f21fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas nltk numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UChWsR7O1fku",
        "outputId": "588c9b21-3c6e-41cc-a728-3ea9245dd8de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import nltk\n",
        "import os\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('glove_embeddings'):\n",
        "  !wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "  !unzip glove.6B.zip -d glove_embeddings"
      ],
      "metadata": {
        "id": "H5Qier3D1iSF",
        "outputId": "0e3e8db0-cd00-4949-9af5-d7a54822fecf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-11 15:33:18--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-03-11 15:33:18--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip         35%[======>             ] 287.80M  4.82MB/s    eta 1m 41s "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_DqwdkY1fkv"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhHy7Sby1fkw"
      },
      "outputs": [],
      "source": [
        "dev_set = pd.read_csv('dev.csv')\n",
        "dev_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQlmvS1I1fkw"
      },
      "outputs": [],
      "source": [
        "train_set = pd.read_csv('train.csv')\n",
        "train_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExYfX91t1fkx"
      },
      "outputs": [],
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "    text = nltk.word_tokenize(text)\n",
        "\n",
        "    processed = []\n",
        "    for word in text:\n",
        "        if word in stop_words:\n",
        "            continue\n",
        "\n",
        "        word = lemmatizer.lemmatize(word)\n",
        "\n",
        "        word = word.strip()\n",
        "\n",
        "        if len(word) < 2:\n",
        "            continue\n",
        "\n",
        "        processed.append(word)\n",
        "\n",
        "    return processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqAFgu8l1fky"
      },
      "outputs": [],
      "source": [
        "dev_set['premise_tokens'] = dev_set['premise'].apply(clean_text)\n",
        "dev_set['hypothesis_tokens'] = dev_set['hypothesis'].apply(clean_text)\n",
        "\n",
        "train_set['premise_tokens'] = train_set['premise'].apply(clean_text)\n",
        "train_set['hypothesis_tokens'] = train_set['hypothesis'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRmvOcx11fkz"
      },
      "outputs": [],
      "source": [
        "dev_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd8StseU1fkz"
      },
      "outputs": [],
      "source": [
        "train_set.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9JrXj2d1fk1"
      },
      "source": [
        "Dataset analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmG2nSeG1fk1"
      },
      "outputs": [],
      "source": [
        "# Labels = dev_set['label'].unique()\n",
        "# Labels\n",
        "\n",
        "# def get_word_frequency(data):\n",
        "#     word_freq = {}\n",
        "#     for row in data:\n",
        "#         for word in row:\n",
        "#             if word in word_freq:\n",
        "#                 word_freq[word] += 1\n",
        "#             else:\n",
        "#                 word_freq[word] = 1\n",
        "#     return word_freq\n",
        "\n",
        "# word_freq = get_word_frequency(train_set['premise_tokens'] + train_set['hypothesis'])\n",
        "\n",
        "# # nltk FreqDist\n",
        "# from nltk import FreqDist\n",
        "\n",
        "# fdist = FreqDist(word_freq)\n",
        "# fdist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPJR8Bl-1fk1"
      },
      "source": [
        "# embeddings/ vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTLuOgi81fk1"
      },
      "outputs": [],
      "source": [
        "glove = \"./glove_embeddings/glove.6B.200d.txt\"\n",
        "def load_glove(glove_file):\n",
        "    embeddings_dict = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype=np.float32)  # <-- Convert to float32\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict\n",
        "\n",
        "embedding_dim = 200\n",
        "loaded_glove = load_glove(glove)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l1pjn5z1fk1"
      },
      "outputs": [],
      "source": [
        "def sentence_embedding(tokens, embeddings_dict, embedding_dim):\n",
        "    print(\"part 2\")\n",
        "    valid_embeddings = [embeddings_dict[token] for token in tokens if token in embeddings_dict]\n",
        "    print(\"part 3\")\n",
        "\n",
        "    if not valid_embeddings:\n",
        "        # Return zero-vector if no embeddings found\n",
        "        return np.zeros(embedding_dim)\n",
        "    print(\"part 4\")\n",
        "    print(tokens)\n",
        "    print(valid_embeddings)\n",
        "    sentence_emb = np.mean(valid_embeddings, axis=0)\n",
        "    return sentence_emb\n",
        "\n",
        "def pairwise_embedding(premise_tokens, hypothesis_tokens, embeddings_dict,embedding_dim):\n",
        "    print(\"part 1\")\n",
        "    premise_emb = sentence_embedding(premise_tokens, embeddings_dict,embedding_dim)\n",
        "    print(\"part 4.5\")\n",
        "    hypothesis_emb = sentence_embedding(hypothesis_tokens, embeddings_dict,embedding_dim)\n",
        "    print(\"part 5\")\n",
        "    # Concatenate multiple useful features\n",
        "    combined_emb = np.concatenate([\n",
        "        premise_emb,\n",
        "        hypothesis_emb,\n",
        "        np.abs(premise_emb - hypothesis_emb), # capture difference\n",
        "        premise_emb * hypothesis_emb           # capture interactions\n",
        "    ]).astype(np.float32)\n",
        "\n",
        "    return combined_emb\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9GWj68p1fk2"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# Make sure premise and hypothesis columns contain lists of tokens\n",
        "train_set['combined_embedding'] = train_set.apply(\n",
        "    lambda x: pairwise_embedding(x['premise_tokens'], x['hypothesis_tokens'], loaded_glove, embedding_dim),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "dev_set['combined_embedding'] = dev_set.apply(\n",
        "    lambda x: pairwise_embedding(x['premise_tokens'], x['hypothesis_tokens'], loaded_glove, embedding_dim),\n",
        "    axis=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_68rjHdA1fk2"
      },
      "outputs": [],
      "source": [
        "train_set.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0alJR9Dd1fk2"
      },
      "source": [
        "# Traditional Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puV0EXHm1fk2"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(train_set['combined_embedding'], train_set['label'])\n",
        "\n",
        "# Evaluate on validation set\n",
        "preds = clf.predict(dev_set['combined_embedding'])\n",
        "print(classification_report(dev_set['label'], preds, target_names=['entailment', 'neutral', 'contradiction']))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}