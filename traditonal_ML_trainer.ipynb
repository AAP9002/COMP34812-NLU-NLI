{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP34812 Natural Language Understanding Courseworklow key lemming an stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas nltk numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zaccu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zaccu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By starting at the soft underbelly, the 16,000...</td>\n",
       "      <td>General Nelson A. Miles had 30,000 troops in h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The class had broken into a light sweat, but w...</td>\n",
       "      <td>The class grew more tense as time went on.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Samson had his famous haircut here, but he wou...</td>\n",
       "      <td>It was unknown where exactly within the town S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man with a black shirt holds a baby while a ...</td>\n",
       "      <td>A darkly dressed man passes a crying baby to a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I know that many of you are interested in addr...</td>\n",
       "      <td>The problems must be addressed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  By starting at the soft underbelly, the 16,000...   \n",
       "1  The class had broken into a light sweat, but w...   \n",
       "2  Samson had his famous haircut here, but he wou...   \n",
       "3  A man with a black shirt holds a baby while a ...   \n",
       "4  I know that many of you are interested in addr...   \n",
       "\n",
       "                                          hypothesis  label  \n",
       "0  General Nelson A. Miles had 30,000 troops in h...      0  \n",
       "1         The class grew more tense as time went on.      1  \n",
       "2  It was unknown where exactly within the town S...      1  \n",
       "3  A darkly dressed man passes a crying baby to a...      0  \n",
       "4                    The problems must be addressed       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set = pd.read_csv('dev.csv')\n",
    "dev_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yeah i don't know cut California in half or so...</td>\n",
       "      <td>Yeah. I'm not sure how to make that fit. Maybe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actual names will not be used</td>\n",
       "      <td>For the sake of privacy, actual names are not ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The film was directed by Randall Wallace.</td>\n",
       "      <td>The film was directed by Randall Wallace and s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"How d'you know he'll sign me on?\"Anse studie...</td>\n",
       "      <td>Anse looked at himself in a cracked mirror.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the light of the candles his cheeks looked ...</td>\n",
       "      <td>Drew regarded his best friend and noted that i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  yeah i don't know cut California in half or so...   \n",
       "1                      actual names will not be used   \n",
       "2          The film was directed by Randall Wallace.   \n",
       "3   \"How d'you know he'll sign me on?\"Anse studie...   \n",
       "4  In the light of the candles his cheeks looked ...   \n",
       "\n",
       "                                          hypothesis  label  \n",
       "0  Yeah. I'm not sure how to make that fit. Maybe...      1  \n",
       "1  For the sake of privacy, actual names are not ...      1  \n",
       "2  The film was directed by Randall Wallace and s...      1  \n",
       "3       Anse looked at himself in a cracked mirror.       1  \n",
       "4  Drew regarded his best friend and noted that i...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = pd.read_csv('train.csv')\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "    text = nltk.word_tokenize(text)\n",
    "\n",
    "    processed = []\n",
    "    for word in text:\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "        word = word.strip()\n",
    "\n",
    "        if len(word) < 2:\n",
    "            continue\n",
    "\n",
    "        processed.append(word)\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set['premise_tokens'] = dev_set['premise'].apply(clean_text)\n",
    "dev_set['hypothesis_tokens'] = dev_set['hypothesis'].apply(clean_text)\n",
    "\n",
    "train_set['premise_tokens'] = train_set['premise'].apply(clean_text)\n",
    "train_set['hypothesis_tokens'] = train_set['hypothesis'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>premise_tokens</th>\n",
       "      <th>hypothesis_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By starting at the soft underbelly, the 16,000...</td>\n",
       "      <td>General Nelson A. Miles had 30,000 troops in h...</td>\n",
       "      <td>0</td>\n",
       "      <td>[starting, soft, underbelly, 16, 000, troop, g...</td>\n",
       "      <td>[general, nelson, mile, 30, 000, troop, attack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The class had broken into a light sweat, but w...</td>\n",
       "      <td>The class grew more tense as time went on.</td>\n",
       "      <td>1</td>\n",
       "      <td>[class, broken, light, sweat, gasping, air]</td>\n",
       "      <td>[class, grew, tense, time, went]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Samson had his famous haircut here, but he wou...</td>\n",
       "      <td>It was unknown where exactly within the town S...</td>\n",
       "      <td>1</td>\n",
       "      <td>[samson, famous, haircut, would, find, hard, r...</td>\n",
       "      <td>[unknown, exactly, within, town, samson, recei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man with a black shirt holds a baby while a ...</td>\n",
       "      <td>A darkly dressed man passes a crying baby to a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[man, black, shirt, hold, baby, blue, shirted,...</td>\n",
       "      <td>[darkly, dressed, man, pass, cry, baby, man, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I know that many of you are interested in addr...</td>\n",
       "      <td>The problems must be addressed</td>\n",
       "      <td>1</td>\n",
       "      <td>[know, many, interested, addressing, issue, le...</td>\n",
       "      <td>[problem, must, addressed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  By starting at the soft underbelly, the 16,000...   \n",
       "1  The class had broken into a light sweat, but w...   \n",
       "2  Samson had his famous haircut here, but he wou...   \n",
       "3  A man with a black shirt holds a baby while a ...   \n",
       "4  I know that many of you are interested in addr...   \n",
       "\n",
       "                                          hypothesis  label  \\\n",
       "0  General Nelson A. Miles had 30,000 troops in h...      0   \n",
       "1         The class grew more tense as time went on.      1   \n",
       "2  It was unknown where exactly within the town S...      1   \n",
       "3  A darkly dressed man passes a crying baby to a...      0   \n",
       "4                    The problems must be addressed       1   \n",
       "\n",
       "                                      premise_tokens  \\\n",
       "0  [starting, soft, underbelly, 16, 000, troop, g...   \n",
       "1        [class, broken, light, sweat, gasping, air]   \n",
       "2  [samson, famous, haircut, would, find, hard, r...   \n",
       "3  [man, black, shirt, hold, baby, blue, shirted,...   \n",
       "4  [know, many, interested, addressing, issue, le...   \n",
       "\n",
       "                                   hypothesis_tokens  \n",
       "0    [general, nelson, mile, 30, 000, troop, attack]  \n",
       "1                   [class, grew, tense, time, went]  \n",
       "2  [unknown, exactly, within, town, samson, recei...  \n",
       "3  [darkly, dressed, man, pass, cry, baby, man, l...  \n",
       "4                         [problem, must, addressed]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>premise_tokens</th>\n",
       "      <th>hypothesis_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yeah i don't know cut California in half or so...</td>\n",
       "      <td>Yeah. I'm not sure how to make that fit. Maybe...</td>\n",
       "      <td>1</td>\n",
       "      <td>[yeah, know, cut, california, half, something]</td>\n",
       "      <td>[yeah, sure, make, fit, maybe, could, cut, cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actual names will not be used</td>\n",
       "      <td>For the sake of privacy, actual names are not ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[actual, name, used]</td>\n",
       "      <td>[sake, privacy, actual, name, used]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The film was directed by Randall Wallace.</td>\n",
       "      <td>The film was directed by Randall Wallace and s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[film, directed, randall, wallace]</td>\n",
       "      <td>[film, directed, randall, wallace, star, mel, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"How d'you know he'll sign me on?\"Anse studie...</td>\n",
       "      <td>Anse looked at himself in a cracked mirror.</td>\n",
       "      <td>1</td>\n",
       "      <td>[know, sign, anse, studied, unkempt, clean, re...</td>\n",
       "      <td>[anse, looked, cracked, mirror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the light of the candles his cheeks looked ...</td>\n",
       "      <td>Drew regarded his best friend and noted that i...</td>\n",
       "      <td>1</td>\n",
       "      <td>[light, candle, cheek, looked, even, hollow, t...</td>\n",
       "      <td>[drew, regarded, best, friend, noted, light, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  yeah i don't know cut California in half or so...   \n",
       "1                      actual names will not be used   \n",
       "2          The film was directed by Randall Wallace.   \n",
       "3   \"How d'you know he'll sign me on?\"Anse studie...   \n",
       "4  In the light of the candles his cheeks looked ...   \n",
       "\n",
       "                                          hypothesis  label  \\\n",
       "0  Yeah. I'm not sure how to make that fit. Maybe...      1   \n",
       "1  For the sake of privacy, actual names are not ...      1   \n",
       "2  The film was directed by Randall Wallace and s...      1   \n",
       "3       Anse looked at himself in a cracked mirror.       1   \n",
       "4  Drew regarded his best friend and noted that i...      1   \n",
       "\n",
       "                                      premise_tokens  \\\n",
       "0     [yeah, know, cut, california, half, something]   \n",
       "1                               [actual, name, used]   \n",
       "2                 [film, directed, randall, wallace]   \n",
       "3  [know, sign, anse, studied, unkempt, clean, re...   \n",
       "4  [light, candle, cheek, looked, even, hollow, t...   \n",
       "\n",
       "                                   hypothesis_tokens  \n",
       "0  [yeah, sure, make, fit, maybe, could, cut, cal...  \n",
       "1                [sake, privacy, actual, name, used]  \n",
       "2  [film, directed, randall, wallace, star, mel, ...  \n",
       "3                    [anse, looked, cracked, mirror]  \n",
       "4  [drew, regarded, best, friend, noted, light, l...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels = dev_set['label'].unique()\n",
    "# Labels\n",
    "\n",
    "# def get_word_frequency(data):\n",
    "#     word_freq = {}\n",
    "#     for row in data:\n",
    "#         for word in row:\n",
    "#             if word in word_freq:\n",
    "#                 word_freq[word] += 1\n",
    "#             else:\n",
    "#                 word_freq[word] = 1\n",
    "#     return word_freq\n",
    "\n",
    "# word_freq = get_word_frequency(train_set['premise_tokens'] + train_set['hypothesis'])\n",
    "\n",
    "# # nltk FreqDist\n",
    "# from nltk import FreqDist\n",
    "\n",
    "# fdist = FreqDist(word_freq)\n",
    "# fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings/ vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = \"./glove_embeddings/glove.6B.200d.txt\"\n",
    "def load_glove(glove_file):\n",
    "    embeddings_dict = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)  # <-- Convert to float32\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "embedding_dim = 200\n",
    "loaded_glove = load_glove(glove)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(tokens, embeddings_dict, embedding_dim):\n",
    "    print(\"part 2\")\n",
    "    valid_embeddings = [embeddings_dict[token] for token in tokens if token in embeddings_dict]\n",
    "    print(\"part 3\")\n",
    "\n",
    "    if not valid_embeddings:\n",
    "        # Return zero-vector if no embeddings found\n",
    "        return np.zeros(embedding_dim)\n",
    "    print(\"part 4\")\n",
    "    print(tokens)\n",
    "    print(valid_embeddings)\n",
    "    sentence_emb = np.mean(valid_embeddings, axis=0)\n",
    "    return sentence_emb\n",
    "\n",
    "def pairwise_embedding(premise_tokens, hypothesis_tokens, embeddings_dict,embedding_dim):\n",
    "    print(\"part 1\")\n",
    "    premise_emb = sentence_embedding(premise_tokens, embeddings_dict,embedding_dim)\n",
    "    print(\"part 4.5\")\n",
    "    hypothesis_emb = sentence_embedding(hypothesis_tokens, embeddings_dict,embedding_dim)\n",
    "    print(\"part 5\")\n",
    "    # Concatenate multiple useful features\n",
    "    combined_emb = np.concatenate([\n",
    "        premise_emb,\n",
    "        hypothesis_emb,\n",
    "        np.abs(premise_emb - hypothesis_emb), # capture difference\n",
    "        premise_emb * hypothesis_emb           # capture interactions\n",
    "    ]).astype(np.float32) \n",
    "    \n",
    "    return combined_emb \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 1\n",
      "part 2\n",
      "part 3\n",
      "part 4\n",
      "['yeah', 'know', 'cut', 'california', 'half', 'something']\n",
      "[['0.58852', '0.23437', '0.015102', '0.1542', '-0.66995', '0.22334', '-0.59206', '0.25223', '0.21026', '0.54238', '-0.44113', '0.043027', '0.40226', '0.26381', '-0.55256', '0.46819', '-0.92069', '0.34615', '0.41744', '0.029842', '0.56711', '0.14103', '-0.049243', '-0.40423', '-0.12173', '-0.88096', '0.15899', '0.24584', '-0.45463', '-0.61556', '-0.51147', '-0.32836', '-0.3162', '-0.2171', '-0.29038', '-0.11232', '-0.0081165', '-0.38394', '-0.19524', '0.5495', '-0.60194', '-0.22716', '-0.51569', '0.29571', '0.10878', '-0.29637', '0.64856', '-0.10606', '-0.19638', '0.090679', '0.73177', '0.11545', '-0.15148', '0.062442', '-0.2977', '-0.091699', '-0.034336', '0.19295', '-0.65724', '-0.36813', '0.23374', '-0.3187', '-0.090369', '0.098264', '-0.16097', '-0.15603', '-0.025413', '0.26073', '0.59589', '-0.1371', '0.43492', '-0.90516', '-0.10999', '-0.43928', '-0.47039', '-0.85725', '-0.55316', '0.80309', '-0.63625', '-0.1858', '0.53891', '0.14227', '-0.082552', '0.37083', '-0.51432', '-0.5519', '-0.054127', '-0.24471', '-0.19309', '-0.91457', '0.30748', '-0.32602', '0.29635', '-0.21281', '0.10678', '0.22008', '0.10904', '0.0015995', '-0.41826', '-0.12245', '0.0093848', '0.44199', '0.086388', '-0.28869', '0.44244', '0.2061', '-0.6449', '1.0482', '-0.18606', '0.71449', '-0.013616', '0.59046', '-0.0087188', '0.11995', '-0.83424', '0.077528', '0.15292', '-0.14149', '-0.29631', '0.10176', '0.17527', '-0.061116', '0.34163', '-0.26578', '-0.34039', '-0.1083', '0.17416', '0.058043', '0.52332', '-0.70546', '-0.42465', '-0.29122', '-0.35402', '0.15193', '-0.42784', '0.033316', '0.15632', '0.13147', '-0.0036998', '-0.30174', '-0.15108', '0.14268', '0.13656', '-0.44931', '0.917', '0.3603', '0.47997', '-0.55015', '-0.10965', '-0.088776', '0.54565', '1.0538', '-0.052646', '0.70599', '0.21756', '-0.18748', '-0.24336', '-0.267', '0.22757', '0.35773', '-0.092579', '-0.14909', '0.00081623', '-0.44793', '-0.404', '-0.63634', '0.24425', '0.29249', '-0.25121', '-1.1337', '0.1172', '0.19455', '0.77738', '0.022317', '0.080097', '0.0064765', '-0.55256', '0.12123', '0.10611', '0.18803', '0.5697', '-0.082379', '-0.073922', '0.33134', '-0.34798', '-0.74082', '0.5487', '0.27665', '0.051156', '-0.52762', '-0.096028', '-0.10691', '0.37381', '0.012454', '-0.15412', '-0.0061703', '0.0092236', '0.056616', '-0.037464', '0.51246'], ['0.43934', '0.28488', '0.1432', '-0.11852', '-0.27219', '0.34972', '-1.0859', '0.35062', '0.31022', '0.51066', '-0.24914', '0.22974', '0.17571', '0.030752', '-0.27839', '0.50097', '-0.048916', '0.40074', '-0.19172', '0.02461', '0.24021', '2.6897', '-0.22387', '0.037135', '0.20814', '-0.40887', '0.017446', '0.044193', '-0.049612', '-0.17237', '-0.056826', '-0.079758', '-0.20414', '-0.045933', '0.018829', '-0.40368', '-0.61027', '-0.14547', '0.00025633', '0.4658', '-0.066146', '-0.17312', '-0.18576', '0.12363', '-0.44447', '0.19166', '0.74497', '-0.1124', '0.21541', '0.050848', '0.16599', '-0.12729', '-0.18245', '0.25725', '0.44189', '-0.15629', '0.13655', '-0.27922', '0.020682', '0.16052', '0.19546', '0.23507', '-0.26915', '0.064108', '-0.18009', '-0.32088', '-0.11522', '0.37638', '0.33827', '0.33295', '0.65328', '-0.043588', '-0.024374', '-0.087489', '-0.52959', '-0.54208', '-0.75417', '0.15358', '-0.30937', '0.11089', '0.10991', '0.40307', '-0.15805', '0.34425', '0.016629', '-0.19079', '-0.5315', '-0.051247', '0.34327', '-1.4198', '0.68124', '-0.19032', '0.41645', '-0.31459', '-0.30357', '0.34647', '0.098532', '0.17199', '-0.16688', '-0.032292', '0.42557', '0.1046', '-0.3667', '-0.26462', '0.32698', '0.03237', '-0.11404', '1.4929', '-0.30914', '0.22148', '0.37393', '0.11722', '0.286', '-0.04138', '-0.14829', '-0.059703', '0.067637', '-0.054731', '-0.38007', '0.15624', '0.75219', '-0.0029463', '0.57392', '-0.27819', '-0.17822', '-0.04907', '0.017259', '-0.23465', '0.35228', '-0.30982', '-0.5004', '0.19552', '-0.14675', '-0.0048649', '-0.20857', '-0.12807', '0.098773', '0.043893', '-0.25081', '-0.31166', '-0.10505', '-0.10176', '0.16119', '-0.34811', '1.5238', '0.47018', '0.33942', '-0.63165', '0.15214', '0.11486', '0.78236', '0.50897', '-0.46711', '0.26841', '-0.032198', '-0.18619', '0.079028', '-0.10166', '0.014734', '0.58065', '-0.26302', '0.14064', '0.36121', '-0.29114', '-0.062966', '0.017865', '-0.080748', '0.27375', '-0.45209', '-0.51138', '0.10315', '0.0031873', '0.11031', '0.16868', '0.3802', '-0.24259', '-0.29028', '0.14141', '0.35058', '0.23117', '0.96703', '-0.29552', '-0.35316', '-0.27632', '-0.37017', '-0.50013', '0.40156', '0.22041', '0.16929', '0.014543', '-0.20884', '0.0016602', '0.029727', '0.21654', '-0.057037', '0.024677', '0.12763', '0.26571', '-0.02378', '0.11601'], ['-0.17936', '-0.5468', '-0.4067', '0.063237', '0.5207', '0.074955', '-0.13326', '-0.69743', '-0.22968', '0.30998', '-0.15981', '-0.0067149', '0.23453', '-0.1505', '0.25438', '-0.30214', '-0.30449', '0.033102', '0.41622', '-0.12676', '-0.20851', '2.8025', '-0.0037314', '-0.31399', '0.18614', '0.72622', '-0.14423', '-0.0737', '-0.85357', '-0.034868', '-0.26478', '-0.14921', '0.29939', '0.55022', '0.017107', '-0.092228', '-0.40354', '-0.61646', '0.37207', '0.078591', '0.53622', '-0.3516', '0.13317', '0.35845', '-0.17771', '0.077107', '0.112', '0.3177', '-0.64623', '-0.28944', '0.76468', '0.1463', '-0.3486', '0.089546', '0.26093', '-0.2416', '-0.9693', '-0.29265', '-0.0011429', '-0.19701', '0.15955', '0.6983', '-0.18', '-0.08967', '0.099372', '0.38474', '0.39332', '0.31564', '0.27199', '-0.69118', '0.18831', '0.28984', '0.032391', '0.045262', '-0.22593', '1.2446', '0.0064386', '-0.12057', '-0.10437', '-0.13539', '0.039318', '-0.60106', '0.14847', '-0.18935', '0.35575', '-0.28058', '0.06677', '-0.68544', '0.20506', '-1.1256', '0.12718', '-0.10339', '-0.18913', '0.53042', '-0.31368', '0.19862', '-0.12701', '0.13937', '-0.14219', '-0.28749', '0.099418', '0.15324', '0.23112', '-0.007951', '-0.1268', '0.45827', '-0.46143', '1.1303', '-0.20637', '-0.68244', '-0.73808', '0.18291', '0.07754', '-0.11088', '-0.028123', '-0.47978', '0.060057', '0.1153', '0.11239', '-0.034916', '0.55686', '-0.011428', '0.11935', '0.30686', '0.036251', '-0.29303', '0.34823', '0.052925', '0.35397', '0.041212', '-0.37388', '-0.15816', '-0.42666', '-0.1364', '0.37571', '0.47896', '0.19091', '-0.1489', '-0.35662', '-0.42718', '-0.13129', '-0.31436', '0.6103', '-0.46294', '0.7246', '0.46872', '0.15237', '-0.50933', '-0.17895', '0.18236', '-0.21759', '0.34863', '0.51369', '0.17253', '-0.5122', '-0.21843', '0.07968', '0.44469', '0.59469', '-0.51734', '0.24741', '0.0061309', '0.36109', '-0.06578', '-0.51169', '0.40014', '0.24926', '0.13655', '0.29412', '0.15583', '0.033774', '-0.032514', '0.1578', '0.26364', '0.12387', '0.68697', '0.18803', '-0.27942', '0.53018', '0.1153', '0.93009', '-0.17574', '0.095503', '0.76412', '-0.18668', '-0.40878', '-0.53858', '-0.24069', '-0.045186', '0.43649', '-0.12978', '0.05547', '-0.45777', '-0.17486', '0.26378', '-0.18781', '-0.299', '0.2751', '-0.51585', '0.72297'], ['-0.55001', '0.35613', '-0.19755', '-0.92031', '0.40404', '0.27082', '0.053192', '0.13732', '3.2572e-05', '-0.4953', '0.2096', '0.19625', '-0.00050789', '0.22391', '0.42521', '0.21661', '0.17629', '0.11578', '0.20308', '-0.13665', '0.32927', '2.221', '-0.3232', '0.11763', '0.81029', '0.89593', '0.30054', '-0.10313', '0.36495', '-0.02301', '0.10062', '-0.53239', '0.22595', '0.296', '-0.39454', '0.18343', '0.056747', '0.34298', '0.83953', '0.3319', '0.077416', '-0.053135', '-0.34244', '0.15188', '-0.12499', '0.047049', '0.39718', '-0.33043', '0.0067798', '0.50756', '-0.093057', '0.27416', '0.070936', '0.31582', '0.12076', '0.1227', '-0.68815', '-0.3741', '-0.075926', '-0.064148', '0.58227', '0.23009', '-0.58509', '0.26331', '0.092735', '-0.48454', '-0.13188', '0.29746', '0.98759', '-0.098119', '0.10294', '0.51843', '0.063906', '0.216', '-0.045815', '0.27314', '0.25421', '0.63479', '-0.12689', '-0.32792', '-0.12906', '-0.20812', '-0.26119', '0.32182', '-0.1373', '-0.68817', '-0.74557', '-0.55277', '0.9084', '-0.59822', '-0.30953', '0.37655', '-0.48964', '0.24876', '0.09039', '-0.012301', '0.29569', '-0.49397', '0.61861', '-0.043744', '0.1101', '-0.030549', '0.13442', '-0.10458', '0.74127', '-0.68697', '0.03045', '2.0349', '-0.21275', '-0.54387', '0.6814', '0.57485', '0.72181', '0.52648', '-0.33945', '-0.18239', '-0.30734', '-0.10062', '-0.74355', '-0.72161', '0.056229', '-0.86181', '-0.50724', '-0.21086', '0.67628', '-0.14125', '0.076337', '-0.0041283', '-0.042594', '-0.080007', '-0.19879', '-0.0016646', '0.71864', '-0.027838', '0.55359', '-0.084541', '0.31884', '0.39567', '-0.14263', '0.32579', '0.20588', '0.34354', '-0.31438', '0.37792', '0.9059', '0.44958', '0.49847', '0.31526', '0.25832', '0.44334', '-0.14473', '-0.13098', '-0.48177', '-0.47683', '0.6712', '-0.10145', '0.031127', '0.29045', '0.44427', '-0.44357', '-0.39766', '-0.2127', '-0.44915', '0.40878', '-0.13493', '0.98119', '0.53247', '-0.49901', '0.23121', '0.61864', '0.27728', '-0.031362', '0.39489', '0.92938', '-0.26434', '-0.38515', '0.72343', '0.13343', '0.43776', '0.33797', '0.31939', '0.62335', '-0.38022', '0.48803', '-0.27629', '0.54385', '-0.040979', '0.20122', '-0.089742', '0.39775', '0.03822', '-0.29309', '-0.016594', '-0.49605', '-0.67847', '0.66428', '-0.5586', '-0.073409', '0.17531', '-0.074713'], ['-0.44575', '-0.57158', '-0.099773', '-0.014698', '0.010865', '-0.18742', '-0.16862', '-0.29417', '-0.022848', '0.55937', '0.028077', '0.1296', '0.016847', '0.01416', '0.069936', '0.12179', '0.44584', '-0.34482', '-0.12661', '-0.37294', '0.12772', '2.6929', '-0.25074', '-0.44717', '0.26314', '0.021171', '-0.33625', '0.10441', '0.045014', '-0.24023', '0.25748', '-0.29257', '-0.076051', '-0.086076', '-0.11259', '-0.18385', '-0.41915', '-0.68445', '-0.080799', '-0.049884', '0.084638', '-0.14818', '0.23811', '0.16228', '-0.32462', '0.72622', '0.20154', '0.23699', '-0.61139', '-0.18405', '-0.24864', '0.17613', '-0.32089', '0.43726', '0.61131', '0.41115', '-0.4151', '-0.075647', '-0.012144', '-0.29136', '-0.38116', '-0.050338', '-0.66071', '-0.21152', '-0.13324', '0.48653', '0.16307', '0.0014308', '0.060094', '-0.8523', '0.16158', '0.26271', '0.24358', '0.49242', '0.072544', '0.63537', '-0.20025', '0.13378', '-0.39181', '0.30049', '0.21493', '-0.40513', '0.13376', '-0.37312', '0.37444', '-0.17764', '0.14291', '-0.4066', '-0.084557', '-0.26369', '0.36507', '0.02265', '-0.10695', '0.062852', '0.30714', '0.39429', '-0.17088', '-0.12193', '0.11443', '-0.15523', '-0.014087', '-0.32104', '0.13274', '0.29781', '-0.34997', '0.1148', '0.35799', '1.6477', '-0.84268', '-0.68185', '0.14269', '0.28042', '-0.24981', '0.43537', '0.43751', '-0.019286', '-0.11566', '-0.14768', '0.13885', '0.0087293', '-0.6001', '-0.24277', '0.10198', '0.16236', '-0.019289', '-0.85512', '0.088497', '-0.15713', '0.81442', '0.35379', '-0.02779', '-0.48183', '0.27842', '-0.36953', '0.36061', '-0.083484', '0.30671', '-0.291', '-0.58005', '-0.10302', '0.015616', '0.023191', '-0.02333', '-0.2057', '0.98827', '0.27946', '0.24698', '-0.35698', '-0.18452', '-0.066256', '-0.040111', '0.24852', '0.26852', '0.12368', '0.46663', '-0.17791', '-0.03764', '-0.54708', '0.2838', '-0.33211', '-0.0095895', '-0.11462', '0.67081', '0.087651', '-0.10434', '0.5178', '-0.10664', '0.074221', '-0.4992', '0.095308', '-0.51678', '0.087587', '-0.15472', '0.52596', '-0.25358', '0.54697', '-0.11294', '-0.17582', '-0.14757', '0.077758', '1.4529', '-0.076336', '-0.61039', '0.6599', '0.0041419', '-0.54219', '-0.33306', '-0.035573', '-0.24743', '0.17919', '-0.25267', '0.18178', '-0.047881', '-0.073458', '0.18005', '-0.22524', '-0.1194', '0.56334', '-0.25713', '0.54874'], ['0.47241', '0.1348', '-0.308', '-0.38037', '-0.050223', '-0.17769', '-0.65435', '0.077492', '0.5481', '0.40445', '-0.2021', '0.5253', '0.24836', '-0.18132', '0.055969', '0.069195', '-0.050195', '0.49772', '0.16974', '-0.20894', '0.16659', '2.4535', '-0.1769', '0.18831', '0.206', '-0.41079', '0.23757', '-0.0088489', '-0.11169', '-0.24455', '-0.35881', '-0.30782', '0.0079376', '-0.0097143', '-0.24621', '-0.27075', '-0.63403', '-0.1527', '0.0024267', '0.37299', '-0.25578', '0.072947', '0.14815', '0.38708', '-0.25071', '0.12053', '0.82654', '0.25076', '-0.086929', '0.26227', '0.10826', '-0.10061', '-0.082954', '0.32549', '0.37349', '-0.12629', '0.20674', '-0.46491', '-0.054386', '0.036859', '-0.10246', '-0.044074', '-0.18846', '0.15918', '0.11041', '-0.19693', '-0.0052231', '0.45471', '0.31962', '0.1817', '0.5972', '0.14192', '-0.28801', '-0.085903', '-0.66822', '-0.31334', '-0.54652', '-0.15787', '-0.52616', '-0.12153', '0.046512', '-0.028512', '0.2942', '0.2436', '0.34304', '-0.47639', '-0.20093', '-0.13305', '0.60361', '-1.0196', '0.41653', '-0.12247', '0.28018', '0.077104', '-0.46254', '-0.0061674', '-0.11819', '0.031368', '-0.42078', '0.12199', '0.37714', '0.64042', '-0.14892', '-0.1238', '0.40499', '0.081172', '-0.26108', '1.2353', '-0.42085', '0.053401', '0.43806', '0.0049905', '0.15966', '-0.32628', '-0.20714', '-0.069745', '-0.44324', '-0.1731', '-0.38416', '0.5373', '0.60244', '-0.022953', '0.46874', '-0.38681', '-0.30356', '-0.11778', '0.079024', '0.28313', '0.19607', '-0.44079', '-0.50245', '0.28943', '0.066221', '-0.2289', '-0.027951', '0.10163', '0.13791', '-0.0072388', '0.055484', '-0.21756', '-0.021938', '-0.11624', '0.24343', '-0.37118', '1.5015', '0.18739', '0.55147', '-0.82221', '0.20071', '0.3849', '0.18798', '0.42493', '-0.71431', '0.46347', '-0.16886', '-0.43038', '-0.074773', '-0.076901', '0.01049', '0.37274', '0.0087618', '-0.15517', '0.28306', '-0.50051', '-0.1568', '-0.64817', '-0.15499', '0.1988', '-0.78619', '-0.2347', '-0.12395', '0.035519', '-0.025137', '0.3531', '0.29772', '-0.23087', '-0.14747', '0.037306', '0.095959', '0.16445', '1.2415', '-0.16394', '-0.17718', '0.014408', '-0.23754', '-0.20034', '0.056214', '0.12821', '0.084153', '0.28885', '-0.2661', '0.1105', '0.055355', '-0.21661', '0.37333', '-0.32239', '-0.15702', '0.49488', '-0.32092', '-0.24197']]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the resolved dtypes are not compatible with add.reduce. Resolved (dtype('<U11'), dtype('<U11'), dtype('<U22'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Make sure premise and hypothesis columns contain lists of tokens\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairwise_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpremise_tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhypothesis_tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_glove\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m dev_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dev_set\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: pairwise_embedding(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpremise_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m], x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhypothesis_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m], loaded_glove, embedding_dim), \n\u001b[0;32m     12\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zaccu\\OneDrive\\Documents\\GitHub\\COMP34812-NLU-NLI\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zaccu\\OneDrive\\Documents\\GitHub\\COMP34812-NLU-NLI\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zaccu\\OneDrive\\Documents\\GitHub\\COMP34812-NLU-NLI\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mc:\\Users\\zaccu\\OneDrive\\Documents\\GitHub\\COMP34812-NLU-NLI\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      2\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Make sure premise and hypothesis columns contain lists of tokens\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_set\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpairwise_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpremise_tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhypothesis_tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_glove\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[0;32m      7\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m dev_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dev_set\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: pairwise_embedding(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpremise_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m], x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhypothesis_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m], loaded_glove, embedding_dim), \n\u001b[0;32m     12\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n",
      "Cell \u001b[1;32mIn[23], line 17\u001b[0m, in \u001b[0;36mpairwise_embedding\u001b[1;34m(premise_tokens, hypothesis_tokens, embeddings_dict, embedding_dim)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpairwise_embedding\u001b[39m(premise_tokens, hypothesis_tokens, embeddings_dict,embedding_dim):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     premise_emb \u001b[38;5;241m=\u001b[39m \u001b[43msentence_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpremise_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart 4.5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m     hypothesis_emb \u001b[38;5;241m=\u001b[39m sentence_embedding(hypothesis_tokens, embeddings_dict,embedding_dim)\n",
      "Cell \u001b[1;32mIn[23], line 12\u001b[0m, in \u001b[0;36msentence_embedding\u001b[1;34m(tokens, embeddings_dict, embedding_dim)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(valid_embeddings)\n\u001b[1;32m---> 12\u001b[0m sentence_emb \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sentence_emb\n",
      "File \u001b[1;32mc:\\Users\\zaccu\\OneDrive\\Documents\\GitHub\\COMP34812-NLU-NLI\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3857\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3858\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3860\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_mean(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   3861\u001b[0m                       out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zaccu\\OneDrive\\Documents\\GitHub\\COMP34812-NLU-NLI\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:135\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    132\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    133\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    137\u001b[0m     ret \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39mtrue_divide(\n\u001b[0;32m    138\u001b[0m             ret, rcount, out\u001b[38;5;241m=\u001b[39mret, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m'\u001b[39m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: the resolved dtypes are not compatible with add.reduce. Resolved (dtype('<U11'), dtype('<U11'), dtype('<U22'))"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Make sure premise and hypothesis columns contain lists of tokens\n",
    "train_set['combined_embedding'] = train_set.apply(\n",
    "    lambda x: pairwise_embedding(x['premise_tokens'], x['hypothesis_tokens'], loaded_glove, embedding_dim), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "dev_set['combined_embedding'] = dev_set.apply(\n",
    "    lambda x: pairwise_embedding(x['premise_tokens'], x['hypothesis_tokens'], loaded_glove, embedding_dim), \n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(train_set['combined_embedding'], train_set['label'])\n",
    "\n",
    "# Evaluate on validation set\n",
    "preds = clf.predict(dev_set['combined_embedding'])\n",
    "print(classification_report(dev_set['label'], preds, target_names=['entailment', 'neutral', 'contradiction']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
