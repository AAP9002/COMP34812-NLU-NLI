{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for Solution A â€“ BiLSTM Model\n",
    "# Run this notebook to generate predictions on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_PATH = 'glove.6B.300d.txt'\n",
    "MODEL_PATH = 'bilstm_model.pt'\n",
    "TEST_PATH = 'test.csv'\n",
    "OUTPUT_PATH = 'predictions.csv'\n",
    "EMBEDDING_DIM = 300\n",
    "seed_value = 42\n",
    "max_len = 80 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requierments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  tensorflow  pandas nltk numpy matplotlib scikit-learn sentencepiece tokenizers\n",
    "!pip install -U spacy[cuda12x]\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install -q gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "import gdown\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PATH = 'dev.csv' # change this to your user data path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download From Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe Embeddings (300D)\n",
    "glove_id = \"1iVUBiXUgN__xN_x0usyXt_otb_RWAenZ\"\n",
    "glove_output = 'glove.6B.300d.txt'\n",
    "if not os.path.exists(glove_output):\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={glove_id}\", glove_output, quiet=False)\n",
    "\n",
    "# Trained BiLSTM Model\n",
    "model_id = \"1-1So2oUrg6U0Hd1r_dl79lXxMs5K0vWZ\"\n",
    "model_output = MODEL_PATH\n",
    "if not os.path.exists(model_output):\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={model_id}\", model_output, quiet=False)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"aap9002/NLI-BILSTM\",\n",
    "    allow_patterns=f\"*\",\n",
    "    local_dir='./'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Keep basic punctuation (.,!?'), remove obscure punctuation\n",
    "    text = re.sub(r\"[^a-z0-9,.!?'\\s]\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenize without removing stopwords or lemmatizing\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove emeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = f\"./glove_embeddings/glove.6B.{EMBEDDING_DIM}d.txt\"\n",
    "def load_glove(glove_file):\n",
    "    embeddings_dict = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)  # <-- Convert to float32\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sequnces emeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_embedding_sequence(tokens, glove, dim):\n",
    "    return [glove.get(tok, np.zeros(dim)) for tok in tokens]\n",
    "def create_sequence_embedding(row, glove, dim, max_len):\n",
    "    premise_seqs = []\n",
    "    hypothesis_seqs = []\n",
    "\n",
    "    for idx, row in row.iterrows():\n",
    "        prem_seq = tokens_to_embedding_sequence(row['premise_tokens'], glove, dim)\n",
    "        hyp_seq = tokens_to_embedding_sequence(row['hypothesis_tokens'], glove, dim)\n",
    "        # Pad separately\n",
    "        prem_seq = pad_sequences([prem_seq], maxlen=max_len, dtype='float32', padding='post', truncating='post')[0]\n",
    "        hyp_seq = pad_sequences([hyp_seq], maxlen=max_len, dtype='float32', padding='post', truncating='post')[0]\n",
    "        if (prem_seq.shape != (max_len, dim) or hyp_seq.shape != (max_len, dim)):\n",
    "            print(\"Heres the issue\")\n",
    "            print(row['premise_tokens'])\n",
    "            print(row['hypothesis_tokens'])\n",
    "        premise_seqs.append(prem_seq)\n",
    "        hypothesis_seqs.append(hyp_seq)\n",
    "\n",
    "    # Explicit casting ensures consistent shape\n",
    "    premise_seqs = np.stack(premise_seqs)\n",
    "    hypothesis_seqs = np.stack(hypothesis_seqs)\n",
    "\n",
    "    return premise_seqs, hypothesis_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(tokens, glove, dim):\n",
    "    valid_embeddings = [glove[token] for token in tokens if token in glove]\n",
    "    if not valid_embeddings:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(valid_embeddings, axis=0)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-5)\n",
    "\n",
    "\n",
    "\n",
    "def extract_verbs(doc):\n",
    "    return set([token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "def precompute_ner_and_verbs(df, nlp):\n",
    "    \"\"\"Precompute NER and verb features for the entire dataframe.\"\"\"\n",
    "    docs1 = list(nlp.pipe(df['premise'].tolist(), batch_size=256))\n",
    "    docs2 = list(nlp.pipe(df['hypothesis'].tolist(), batch_size=256))\n",
    "\n",
    "    ner_features_list = []\n",
    "    verb_features_list = []\n",
    "\n",
    "    for doc1, doc2 in zip(docs1, docs2):\n",
    "        ents1 = set(ent.text.lower() for ent in doc1.ents)\n",
    "        ents2 = set(ent.text.lower() for ent in doc2.ents)\n",
    "        ner_overlap = len(ents1 & ents2) / (len(ents1 | ents2) + 1e-5)\n",
    "\n",
    "        verbs1 = extract_verbs(doc1)\n",
    "        verbs2 = extract_verbs(doc2)\n",
    "        verb_overlap = len(verbs1 & verbs2) / (len(verbs1 | verbs2) + 1e-5)\n",
    "\n",
    "        ner_features_list.append(ner_overlap)\n",
    "        verb_features_list.append(verb_overlap)\n",
    "\n",
    "    return ner_features_list, verb_features_list\n",
    "\n",
    "def prepare_numeric_features_optimized(df, glove, dim, nlp, batch_size=256):\n",
    "    # Precompute NER and Verb features in bulk\n",
    "    ner_features_list, verb_features_list = precompute_ner_and_verbs(df, nlp)\n",
    "\n",
    "    numeric_feats = []\n",
    "    texts1 = df['premise'].tolist()\n",
    "    texts2 = df['hypothesis'].tolist()\n",
    "\n",
    "    # Precompute sentence embeddings\n",
    "    premise_embeddings = [sentence_embedding(tokens, glove, dim) for tokens in df['premise_tokens']]\n",
    "    hypothesis_embeddings = [sentence_embedding(tokens, glove, dim) for tokens in df['hypothesis_tokens']]\n",
    "\n",
    "    for idx in range(len(df)):\n",
    "        prem_emb = premise_embeddings[idx]\n",
    "        hyp_emb = hypothesis_embeddings[idx]\n",
    "        cos_sim = cosine_similarity(prem_emb, hyp_emb)\n",
    "\n",
    "        ner_overlap = ner_features_list[idx]\n",
    "        verb_overlap = verb_features_list[idx]\n",
    "\n",
    "        numeric_feats.append([cos_sim, ner_overlap, verb_overlap])\n",
    "\n",
    "    return np.array(numeric_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(input_csv):\n",
    "    # Load the test data\n",
    "    test_df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Preprocess the text data\n",
    "    test_df['premise_tokens'] = test_df['premise'].apply(clean_text)\n",
    "    test_df['hypothesis_tokens'] = test_df['hypothesis'].apply(clean_text)\n",
    "    # Remove rows where premise or hypothesis are empty\n",
    "    test_df = test_df[test_df['premise'].notna() & test_df['premise'].str.strip().ne('')]\n",
    "    test_df = test_df[test_df['hypothesis'].notna() & test_df['hypothesis'].str.strip().ne('')]\n",
    "\n",
    "    # Load GloVe embeddings\n",
    "    glove = load_glove(GLOVE_PATH)\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = tf.keras.models.load_model('Most_recent_best_esim_model.keras')\n",
    "\n",
    "    # Create sequence embeddings\n",
    "    premise_seqs, hypothesis_seqs = create_sequence_embedding(test_df, glove, EMBEDDING_DIM, max_len)\n",
    "\n",
    "    # Prepare numeric features using optimized function\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    numeric_feats = prepare_numeric_features_optimized(test_df, glove, EMBEDDING_DIM, nlp)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict([premise_seqs, hypothesis_seqs, numeric_feats])\n",
    "\n",
    "    # Convert predictions to binary labels (0 or 1)\n",
    "    binary_predictions = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "    return binary_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_predictions(USER_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
