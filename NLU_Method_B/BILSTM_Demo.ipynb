{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NV4TyOf19wx"
      },
      "source": [
        "# Demo for Solution B â€“ BiLSTM Model\n",
        "\n",
        " Instructions for Using This Script\n",
        "\n",
        "1. Global Parameters  \n",
        "   Do not modify any of the global configuration parameters unless you're sure of what you're doing. These are fixed for consistent model behavior.\n",
        "\n",
        "2. Environment Setup  \n",
        "   Install all required packages by running the installation cell below.  \n",
        "   Note: You may need to restart your environment after installation due to underlying dependency changes.\n",
        "\n",
        "3. User Data Input  \n",
        "   In the 'User Data' section, update the `USER_CSV_PATH` variable with the path to your own CSV file.  \n",
        "   The CSV must contain `premise` and `hypothesis` columns.\n",
        "\n",
        "4. Run All Cells  \n",
        "   Once the path is set and dependencies are installed, run all cells in order.  \n",
        "   This will generate a `predictions.csv` file containing binary predictions (0 for \"Not Entailed\", 1 for \"Entailed\").\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmpp1F7P19wz"
      },
      "source": [
        "# Global Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqAhOv-219wz"
      },
      "outputs": [],
      "source": [
        "GLOVE_PATH = 'glove.6B.300d.txt'\n",
        "MODEL_PATH = 'bilstm_model.pt'\n",
        "TEST_PATH = 'test.csv'\n",
        "OUTPUT_PATH = 'predictions.csv'\n",
        "\n",
        "EMBEDDING_DIM = 300\n",
        "seed_value = 42\n",
        "max_len = 80\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU26VXOu19w1"
      },
      "source": [
        "# Requierments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9-o92j_19w1"
      },
      "outputs": [],
      "source": [
        "!pip install  tensorflow  pandas nltk numpy matplotlib scikit-learn sentencepiece tokenizers --quiet\n",
        "!pip install -U spacy[cuda12x] --quiet\n",
        "!python -m spacy download en_core_web_sm --quiet\n",
        "!pip install -q gdown --quiet\n",
        "#May need to restart run time in notebook/ goole collab due to underlying depencie chnages\n",
        "#--quiet used to reduce output. can be removed for sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyc3NEuC19w1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import nltk\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import spacy\n",
        "import gdown\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK1KgLpF19w1"
      },
      "source": [
        "# Input file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPFZgKhU19w2"
      },
      "outputs": [],
      "source": [
        "USER_PATH = 'dev.csv' # change this to your user data path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t46XBf2J19w2"
      },
      "source": [
        "# Download From Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TWqsAqB19w2"
      },
      "outputs": [],
      "source": [
        "# GloVe Embeddings (300D)\n",
        "glove_id = \"1iVUBiXUgN__xN_x0usyXt_otb_RWAenZ\"\n",
        "glove_output = 'glove.6B.300d.txt'\n",
        "if not os.path.exists(glove_output):\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={glove_id}\", glove_output, quiet=False)\n",
        "\n",
        "snapshot_download(\n",
        "    repo_id=\"aap9002/NLI-BILSTM\",\n",
        "    allow_patterns=f\"*\",\n",
        "    local_dir='./'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiOuMKwA19w3"
      },
      "source": [
        "# Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1HwPfXW19w3"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Keep basic punctuation (.,!?'), remove obscure punctuation\n",
        "    text = re.sub(r\"[^a-z0-9,.!?'\\s]\", ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize without removing stopwords or lemmatizing\n",
        "    text = nltk.word_tokenize(text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_RvfnNZ19w3"
      },
      "source": [
        "# Glove emeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKJGw48r19w3"
      },
      "outputs": [],
      "source": [
        "glove = f\"./glove_embeddings/glove.6B.{EMBEDDING_DIM}d.txt\"\n",
        "def load_glove(glove_file):\n",
        "    embeddings_dict = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype=np.float32)  # <-- Convert to float32\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG-vdcl119w3"
      },
      "source": [
        "# sequnces emeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPLNAdYC19w4"
      },
      "outputs": [],
      "source": [
        "def tokens_to_embedding_sequence(tokens, glove, dim):\n",
        "    return [glove.get(tok, np.zeros(dim)) for tok in tokens]\n",
        "def create_sequence_embedding(row, glove, dim, max_len):\n",
        "    premise_seqs = []\n",
        "    hypothesis_seqs = []\n",
        "\n",
        "    for idx, row in row.iterrows():\n",
        "        prem_seq = tokens_to_embedding_sequence(row['premise_tokens'], glove, dim)\n",
        "        hyp_seq = tokens_to_embedding_sequence(row['hypothesis_tokens'], glove, dim)\n",
        "        # Pad separately\n",
        "        prem_seq = pad_sequences([prem_seq], maxlen=max_len, dtype='float32', padding='post', truncating='post')[0]\n",
        "        hyp_seq = pad_sequences([hyp_seq], maxlen=max_len, dtype='float32', padding='post', truncating='post')[0]\n",
        "        if (prem_seq.shape != (max_len, dim) or hyp_seq.shape != (max_len, dim)):\n",
        "            print(\"Heres the issue\")\n",
        "            print(row['premise_tokens'])\n",
        "            print(row['hypothesis_tokens'])\n",
        "        premise_seqs.append(prem_seq)\n",
        "        hypothesis_seqs.append(hyp_seq)\n",
        "\n",
        "    # Explicit casting ensures consistent shape\n",
        "    premise_seqs = np.stack(premise_seqs)\n",
        "    hypothesis_seqs = np.stack(hypothesis_seqs)\n",
        "\n",
        "    return premise_seqs, hypothesis_seqs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXlri6Cq19w4"
      },
      "source": [
        "# Numerical Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSXz7HOt19w4"
      },
      "outputs": [],
      "source": [
        "def sentence_embedding(tokens, glove, dim):\n",
        "    valid_embeddings = [glove[token] for token in tokens if token in glove]\n",
        "    if not valid_embeddings:\n",
        "        return np.zeros(dim)\n",
        "    return np.mean(valid_embeddings, axis=0)\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-5)\n",
        "\n",
        "\n",
        "\n",
        "def extract_verbs(doc):\n",
        "    return set([token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        "\n",
        "def precompute_ner_and_verbs(df, nlp):\n",
        "    \"\"\"Precompute NER and verb features for the entire dataframe.\"\"\"\n",
        "    docs1 = list(nlp.pipe(df['premise'].tolist(), batch_size=256))\n",
        "    docs2 = list(nlp.pipe(df['hypothesis'].tolist(), batch_size=256))\n",
        "\n",
        "    ner_features_list = []\n",
        "    verb_features_list = []\n",
        "\n",
        "    for doc1, doc2 in zip(docs1, docs2):\n",
        "        ents1 = set(ent.text.lower() for ent in doc1.ents)\n",
        "        ents2 = set(ent.text.lower() for ent in doc2.ents)\n",
        "        ner_overlap = len(ents1 & ents2) / (len(ents1 | ents2) + 1e-5)\n",
        "\n",
        "        verbs1 = extract_verbs(doc1)\n",
        "        verbs2 = extract_verbs(doc2)\n",
        "        verb_overlap = len(verbs1 & verbs2) / (len(verbs1 | verbs2) + 1e-5)\n",
        "\n",
        "        ner_features_list.append(ner_overlap)\n",
        "        verb_features_list.append(verb_overlap)\n",
        "\n",
        "    return ner_features_list, verb_features_list\n",
        "\n",
        "def prepare_numeric_features_optimized(df, glove, dim, nlp):\n",
        "    # Precompute NER and Verb features in bulk\n",
        "    ner_features_list, verb_features_list = precompute_ner_and_verbs(df, nlp)\n",
        "\n",
        "    numeric_feats = []\n",
        "\n",
        "    # Precompute sentence embeddings\n",
        "    premise_embeddings = [sentence_embedding(tokens, glove, dim) for tokens in df['premise_tokens']]\n",
        "    hypothesis_embeddings = [sentence_embedding(tokens, glove, dim) for tokens in df['hypothesis_tokens']]\n",
        "\n",
        "    for idx in range(len(df)):\n",
        "        prem_emb = premise_embeddings[idx]\n",
        "        hyp_emb = hypothesis_embeddings[idx]\n",
        "        cos_sim = cosine_similarity(prem_emb, hyp_emb)\n",
        "\n",
        "        ner_overlap = ner_features_list[idx]\n",
        "        verb_overlap = verb_features_list[idx]\n",
        "\n",
        "        numeric_feats.append([cos_sim, ner_overlap, verb_overlap])\n",
        "\n",
        "    return np.array(numeric_feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d35P8hnY19w4"
      },
      "source": [
        "# Run Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_0PyFg-19w4"
      },
      "outputs": [],
      "source": [
        "def get_predictions(input_csv):\n",
        "    # Load the test data\n",
        "    test_df = pd.read_csv(input_csv)\n",
        "    # Python's built-in random module\n",
        "    random.seed(seed_value)\n",
        "\n",
        "    # NumPy\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "    # TensorFlow\n",
        "    tf.random.set_seed(seed_value)\n",
        "    # Preprocess the text data\n",
        "    test_df['premise_tokens'] = test_df['premise'].apply(clean_text)\n",
        "    test_df['hypothesis_tokens'] = test_df['hypothesis'].apply(clean_text)\n",
        "    # Remove rows where premise or hypothesis are empty\n",
        "    test_df = test_df[test_df['premise'].notna() & test_df['premise'].str.strip().ne('')]\n",
        "    test_df = test_df[test_df['hypothesis'].notna() & test_df['hypothesis'].str.strip().ne('')]\n",
        "\n",
        "    # Load GloVe embeddings\n",
        "    glove = load_glove(GLOVE_PATH)\n",
        "\n",
        "    # Load the trained model\n",
        "    model = tf.keras.models.load_model('Most_recent_best_esim_model.keras')\n",
        "\n",
        "    # Create sequence embeddings\n",
        "    premise_seqs, hypothesis_seqs = create_sequence_embedding(test_df, glove, EMBEDDING_DIM, max_len)\n",
        "\n",
        "    # Prepare numeric features using optimized function\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    numeric_feats = prepare_numeric_features_optimized(test_df, glove, EMBEDDING_DIM, nlp)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict([premise_seqs, hypothesis_seqs, numeric_feats])\n",
        "\n",
        "    # Convert predictions to binary labels (0 or 1)\n",
        "    binary_predictions = (predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "    return binary_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwGZ0ueE19w5"
      },
      "outputs": [],
      "source": [
        "prediction_labels = get_predictions(USER_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-GIM6hc4plu"
      },
      "source": [
        "# Output text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G8oQVzS3vay"
      },
      "outputs": [],
      "source": [
        "columns = ['prediction']\n",
        "df = pd.DataFrame(prediction_labels, columns=columns)\n",
        "df.to_csv(OUTPUT_PATH, index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}